{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfa37f65-48da-4d77-8d1c-ead89e543829",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><h1>Yu-Chieh Chen</h3></center>\n",
    "<center><h4>GitHub Username: ychen221</h4></center>\n",
    "<center><h4>USCID: 6391765959</h4></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156bf13-c7c7-448b-be61-41e536dc0967",
   "metadata": {},
   "source": [
    "## 1. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5823a7-8b3a-49b0-ae0b-564f3ed974f7",
   "metadata": {},
   "source": [
    "### (a) In this problem, we are trying to build a classifier to analyze the sentiment of reviews. You are provided with text data in two folders: one folder involves positive reviews, and one folder involves negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b31459-ce95-4d05-8f7f-f7cd8625b4f6",
   "metadata": {},
   "source": [
    "### (b) Data Exploration and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09977699-de41-4eec-82b5-430a97d2ebf1",
   "metadata": {},
   "source": [
    "#### i.You can use binary encoding for the sentiments , i.e y = 1 for positive senti- ments and y = âˆ’ 1 for negative sentiments. \n",
    "#### ii. The data are pretty clean. Remove the punctuation and numbers from the data. \n",
    "#### iii. The name of each text file starts with cv number . Use text files 0-699 in each class for training and 700-999 for testing. \n",
    "#### iv. Count the number of unique words in the whole dataset (train + test) and print it out. \n",
    "#### v. Calculate the average review length and the standard deviation of review lengths. Report the results. vi. Plot the histogram of review lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28fc8c7-2f67-44da-a686-3f5adeda6a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the dataset: 47037\n",
      "Average review length: 644.36 words\n",
      "Standard deviation of review lengths: 284.98 words\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "def load_data(folder):\n",
    "    \"\"\" Load data from files and preprocess it. \"\"\"\n",
    "    train_texts, test_texts, train_labels, test_labels = [], [], [], []\n",
    "    files = os.listdir(folder)\n",
    "\n",
    "    \n",
    "    regex = re.compile(r'^cv\\d{3}_\\d+\\.txt$')  # Regex to match filenames like 'cv000_29590.txt'\n",
    "    files = [f for f in files if regex.match(f)]\n",
    "\n",
    "    # Sort files based on the numerical part of the filename\n",
    "    files.sort(key=lambda x: int(x.split('_')[0][2:]))\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder, file)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().lower()\n",
    "            text = re.sub(r'[\\d]', '', text)  # Remove numbers\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "\n",
    "            # Extract CV number from filename considering the new pattern\n",
    "            cv_number = int(file.split(\"_\")[0][2:])  # Extract number after 'cv' and before '_'\n",
    "            if 0 <= cv_number <= 699:\n",
    "                train_labels.append(1 if 'pos' in folder else -1)\n",
    "                train_texts.append(text)\n",
    "            elif 700 <= cv_number <= 999:\n",
    "                test_labels.append(1 if 'pos' in folder else -1)  # Assign a placeholder label for test data\n",
    "                test_texts.append(text)\n",
    "    return train_texts, test_texts, train_labels, test_labels\n",
    "\n",
    "train_pos_texts, test_pos_texts, train_pos_labels, test_pos_labels = load_data('../data/pos')\n",
    "train_neg_texts, test_neg_texts, train_neg_labels, test_neg_labels = load_data('../data/neg')\n",
    "\n",
    "# Combine and separate training and testing data\n",
    "train_texts = np.array(train_pos_texts + train_neg_texts)\n",
    "test_texts = np.array(test_pos_texts + test_neg_texts)\n",
    "train_labels = np.array(train_pos_labels + train_neg_labels)\n",
    "test_labels = np.array(test_pos_labels + test_neg_labels)\n",
    "\n",
    "# Combine all texts\n",
    "all_texts = np.concatenate([train_texts, test_texts])\n",
    "\n",
    "# find unique words\n",
    "unique_words = set()\n",
    "for text in all_texts:\n",
    "    words = text.split()\n",
    "    unique_words.update(words)\n",
    "\n",
    "# Print the number of unique words\n",
    "print(f\"Number of unique words in the dataset: {len(unique_words)}\")\n",
    "\n",
    "# Calculate the length of each review\n",
    "review_lengths = [len(text.split()) for text in all_texts]\n",
    "\n",
    "# Calculate the average length and standard deviation\n",
    "average_length = np.mean(review_lengths)\n",
    "std_dev_length = np.std(review_lengths)\n",
    "\n",
    "# Report the results\n",
    "print(f\"Average review length: {average_length:.2f} words\")\n",
    "print(f\"Standard deviation of review lengths: {std_dev_length:.2f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11d77c-b10e-475e-8306-f56404dc7ef1",
   "metadata": {},
   "source": [
    "#### vi.Plot the histogram of review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65fc3a84-477a-412c-9281-3d196cd5ca5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAIhCAYAAABE54vcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABVxklEQVR4nO3de1xVVf7/8fdBEA6IpCICqUSlhpcsL5laiil4SfPSd3QyHW+VZTo56jRj5k+cHC2drMYmc8rQclDrO9llKg3N+6WM0rKItFS8QIYieAFEz/r94XC+nQBl4zkcwNfz8eChe++1P3vtw+LI2733OjZjjBEAAAAAoMx8vN0BAAAAAKhqCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAFBGS5Yskc1m0+eff17i9r59++q6665zWXfddddp5MiRlo6zbds2JSQk6OTJk+Xr6FVo5cqVatGihex2u2w2m3bt2lViuw0bNshmszm/atSoofr166tfv36lfl/dpejYGzZs8OhxLudy49jbjh49qoSEhBK/hyNHjlStWrUqvlMAUAKCFAB40KpVqzR9+nRL+2zbtk0zZ84kSJXRzz//rOHDh+uGG27Q6tWrtX37djVt2vSS+8yePVvbt2/Xhg0bNH36dG3btk1du3bV3r17PdbPNm3aaPv27WrTpo3HjlEdHD16VDNnziw1DANAZeHr7Q4AQHV26623ersLlhUWFspms8nXt2r8E/H999+rsLBQw4YNU9euXcu0T5MmTXT77bdLku68805dc801GjFihJYtW6aZM2d6pJ+1a9d2HhMAUPVxRQoAPOjXt/Y5HA7NmjVLzZo1k91u1zXXXKObb75ZL7zwgiQpISFBf/zjHyVJ0dHRzlvQim4Hczgcmjt3rm666Sb5+/srLCxMv/vd73T48GGX4xpjNHv2bEVFRSkgIEDt2rVTcnKyYmNjFRsb62xXdLvZG2+8ocmTJ+vaa6+Vv7+/9u3bp59//lnjxo1T8+bNVatWLYWFhemuu+7S5s2bXY514MAB2Ww2zZs3T88884yuu+462e12xcbGOkPOn//8Z0VGRiokJEQDBw7UsWPHyvT6vffee+rYsaMCAwMVHBysuLg4bd++3bl95MiRuuOOOyRJQ4YMkc1mczm/smrXrp0k6aeffnJZv3fvXg0dOlRhYWHy9/dXTEyM/vGPfzi3//zzz6pZs2aJVx2/++472Ww2/f3vf5dU+q19n3/+ue655x7VrVtXAQEBuvXWW/Xmm286t+fm5srX11fz5s1zrsvKypKPj49CQkJ0/vx55/rf//73ql+/vowxll+DX7vcuf/ynJYvX65p06YpMjJStWvXVo8ePZSWlubStixjcsOGDWrfvr0kadSoUc7xn5CQ4FJr37596tOnj2rVqqVGjRpp8uTJKigocGmzcOFCtW7dWrVq1VJwcLBuuukmPfHEE1f8ugBAEYIUAFh04cIFnT9/vthXWX55nTt3rhISEnTffffpgw8+0MqVKzVmzBjnbXwPPPCAJkyYIEl6++23tX37dpfbwR555BH96U9/UlxcnN577z099dRTWr16tTp16qSsrCzncaZNm6Zp06apV69eevfdd/Xwww/rgQce0Pfff19iv6ZOnar09HS9/PLLev/99xUWFqYTJ05IkmbMmKEPPvhAiYmJuv766xUbG1vicz7/+Mc/tHXrVv3jH//Qq6++qu+++079+vXTmDFj9PPPP+u1117T3LlztXbtWj3wwAOXfa2SkpLUv39/1a5dW8uXL9fixYuVnZ2t2NhYbdmyRZI0ffp05y/3RbfrvfTSS5et/Wv79++XJJdbAr/99lu1b99ee/bs0bPPPqv//Oc/uvvuu/X73//eedWqfv366tu3r5YuXSqHw+FSMzExUTVr1tT9999f6nHXr1+vzp076+TJk3r55Zf17rvv6pZbbtGQIUO0ZMkSSRevZLVv315r16517rdu3Tr5+/vr1KlT+uyzz5zr165dq7vuuks2m83ya/BLZTn3X3riiSd08OBBvfrqq/rnP/+pvXv3ql+/frpw4YKzTVnGZJs2bZSYmChJevLJJ53j/5fjpbCwUPfcc4+6d++ud999V6NHj9Zzzz2nZ555xtlmxYoVGjdunLp27apVq1bpnXfe0R/+8AedOXPmil4XAHBhAABlkpiYaCRd8isqKspln6ioKDNixAjnct++fc0tt9xyyePMmzfPSDL79+93WZ+ammokmXHjxrms//TTT40k88QTTxhjjDlx4oTx9/c3Q4YMcWm3fft2I8l07drVuW79+vVGkunSpctlz//8+fOmsLDQdO/e3QwcONC5fv/+/UaSad26tblw4YJz/fPPP28kmXvuucelzsSJE40kk5OTU+qxLly4YCIjI02rVq1cap46dcqEhYWZTp06FTuHt95667LnUNR25cqVprCw0Jw9e9Zs3brVNGvWzDRv3txkZ2c72/bs2dM0bNiwWD/Hjx9vAgICzIkTJ4wxxrz33ntGkvn4449dXqvIyEhz7733Fjv2+vXrnetuuukmc+utt5rCwkKXY/Tt29dEREQ4z/3JJ580drvd5OfnG2OMeeCBB0yvXr3MzTffbGbOnGmMMebIkSNGkvnnP/95ydegaBzv3Lmz1DZlPfeic+rTp49LuzfffNNIMtu3bzfGWBuTO3fuNJJMYmJisX6NGDHCSDJvvvmmy/o+ffqYZs2aufTzmmuuKf1FAAA34IoUAFj0+uuva+fOncW+im4xu5TbbrtNu3fv1rhx47RmzRrl5uaW+bjr16+XpGKzAN52222KiYnRunXrJEk7duxQQUGBBg8e7NLu9ttvLzarYJF77723xPUvv/yy2rRpo4CAAPn6+srPz0/r1q1TampqsbZ9+vSRj8///bMSExMjSbr77rtd2hWtT09PL+VMpbS0NB09elTDhw93qVmrVi3de++92rFjh86ePVvq/pczZMgQ+fn5KTAwUJ07d1Zubq4++OADXXPNNZKk/Px8rVu3TgMHDlRgYKDLlcc+ffooPz9fO3bskCT17t1b4eHhzispkrRmzRodPXpUo0ePLrUP+/bt03fffee8YvXrY2RkZDhvj+vevbvy8vK0bds2SRevPMXFxalHjx5KTk52rpOkHj16lPt1sXruRe655x6X5ZtvvlmSdPDgQUnlG5Olsdls6tevX7HjFR1LuvgzcfLkSd1333169913Xa7WAoC7EKQAwKKYmBi1a9eu2FdISMhl9506dar+9re/aceOHerdu7fq1aun7t27l2kq6uPHj0uSIiIiim2LjIx0bi/6s0GDBsXalbSutJrz58/XI488og4dOujf//63duzYoZ07d6pXr17Ky8sr1r5u3bouyzVr1rzk+vz8/BL78stzKO1cHQ6HsrOzS93/cp555hnt3LlTGzdu1LRp0/TTTz9pwIABzudsjh8/rvPnz2vBggXy8/Nz+erTp48kOX859/X11fDhw7Vq1SrnLZpLlixRRESEevbsWWofip7HmjJlSrFjjBs3zuUYnTp1UmBgoNauXat9+/bpwIEDziD16aef6vTp01q7dq2uv/56RUdHl/t1sXruRerVq+ey7O/vL0nOcVKeMVmawMBABQQEFDveL8fT8OHD9dprr+ngwYO69957FRYWpg4dOjhDJwC4Q9WYkgkAqglfX19NmjRJkyZN0smTJ7V27Vo98cQT6tmzpw4dOqTAwMBS9y36ZTUjI0MNGzZ02Xb06FGFhoa6tPv1xAmSlJmZWeIVgJKeqVm2bJliY2O1cOFCl/WnTp269Em6wS/P9deOHj0qHx8f1alTp9z1r7/+eucEE126dJHdbteTTz6pBQsWaMqUKapTp45q1Kih4cOH69FHHy2xxi8Dy6hRozRv3jytWLFCQ4YM0XvvvaeJEyeqRo0apfah6Ps1depUDRo0qMQ2zZo1k3QxfN5xxx1au3atGjZsqPDwcLVq1UrXX3+9pIuTNKxbt059+/a1/mL8itVzL4vyjMkrNWrUKI0aNUpnzpzRpk2bNGPGDPXt21fff/+9oqKi3H48AFcfghQAeMk111yj//mf/9GRI0c0ceJEHThwQM2bNy/2v/lF7rrrLkkXA07RzGaStHPnTqWmpmratGmSpA4dOsjf318rV650+QV9x44dOnjwYJl/abXZbM6+FPnqq6+0fft2NWrUyPL5WtGsWTNde+21SkpK0pQpU5xB78yZM/r3v//tnMnPXR5//HEtWbJETz/9tMaOHavg4GB169ZNX375pW6++WbnVbTSxMTEqEOHDkpMTNSFCxdUUFCgUaNGXfYcmzRpot27d2v27NmX7WOPHj00depUBQcHO2/fCwoK0u23364FCxbo6NGjV3xbn3Txio+Vcy8LK2OytPFfXkFBQerdu7fOnTunAQMG6JtvviFIAXALghQAVKB+/fqpZcuWateunerXr6+DBw/q+eefV1RUlJo0aSJJatWqlSTphRde0IgRI+Tn56dmzZqpWbNmeuihh7RgwQL5+Piod+/eOnDggKZPn65GjRrpD3/4g6SLt9JNmjRJc+bMUZ06dTRw4EAdPnxYM2fOVEREhMszR5fSt29fPfXUU5oxY4a6du2qtLQ0/eUvf1F0dLTLlNue4OPjo7lz5+r+++9X3759NXbsWBUUFGjevHk6efKknn76abcez8/PT7Nnz9bgwYP1wgsv6Mknn9QLL7ygO+64Q3feeaceeeQRXXfddTp16pT27dun999/X5988olLjdGjR2vs2LE6evSoOnXq5LyadCmLFi1S79691bNnT40cOVLXXnutTpw4odTUVH3xxRd66623nG27d++uCxcuaN26dVq6dKlzfY8ePTRjxgzZbDZn2C6LTz75RAcOHCi2vk+fPpbP/XKsjMkbbrhBdrtd//rXvxQTE6NatWopMjJSkZGRZT7egw8+KLvdrs6dOysiIkKZmZmaM2eOQkJCXP4TAgCuiLdnuwCAquJys53dfffdl52179lnnzWdOnUyoaGhpmbNmqZx48ZmzJgx5sCBAy77TZ061URGRhofHx+Xmd4uXLhgnnnmGdO0aVPj5+dnQkNDzbBhw8yhQ4dc9nc4HGbWrFmmYcOGpmbNmubmm282//nPf0zr1q1dZty71Ix3BQUFZsqUKebaa681AQEBpk2bNuadd94xI0aMcDnPoln75s2b57J/abXLMmtckXfeecd06NDBBAQEmKCgINO9e3ezdevWMh2nJJdr26FDB1OnTh1z8uRJ57mNHj3aXHvttcbPz8/Ur1/fdOrUycyaNavYvjk5OcZutxtJ5pVXXin12L+ctc8YY3bv3m0GDx5swsLCjJ+fnwkPDzd33XWXefnll13aORwOExoaaiSZI0eOONdv3brVSDJt2rS57Pkbc/nZJ4tmiyzLuZf2ehaNiV/OvFfWMWmMMcuXLzc33XST8fPzM5LMjBkzjDEXZ+0LCgoqdk4zZswwv/yVZunSpaZbt26mQYMGpmbNmiYyMtIMHjzYfPXVV2V6jQCgLGzGuOFT+wAAld7+/ft10003acaMGXwwKSoFxiSAqowgBQDV0O7du7V8+XJ16tRJtWvXVlpamubOnavc3Fzt2bPH8kxpwJViTAKobnhGCgCqoaCgIH3++edavHixTp48qZCQEMXGxuqvf/0rv7DCKxiTAKobrkgBAAAAgEV8IC8AAAAAWESQAgAAAACLCFIAAAAAYBGTTUhyOBw6evSogoODZbPZvN0dAAAAAF5ijNGpU6cUGRl5yQ+xJ0hJOnr0qBo1auTtbgAAAACoJA4dOqSGDRuWup0gJSk4OFjSxRerdu3aFXbcwsJCffzxx4qPj5efn1+FHRfVG+MKnsLYgicwruAJjCtcidzcXDVq1MiZEUpDkJKct/PVrl27woNUYGCgateuzQ853IZxBU9hbMETGFfwBMYV3OFyj/ww2QQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIt8vd0BAFcuPT1dWVlZkiSHwyFJ2r17t3x83PN/JaGhoWrcuLFbagEAAFQHBCmgiktPT1ezZjHKzz8rSbLb7Vq+fLm6dOmivLw8txwjICBQaWmphCkAAID/IkgBVVxWVtZ/Q9QySTGSHJKOSNok99y9m6r8/GHKysoiSAEAAPwXQQqoNmIktZFUqItBqrUkP6/2CAAAoLpisgkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARV4NUnPmzFH79u0VHByssLAwDRgwQGlpaS5tRo4cKZvN5vJ1++23u7QpKCjQhAkTFBoaqqCgIN1zzz06fPhwRZ4KAAAAgKuIrzcPvnHjRj366KNq3769zp8/r2nTpik+Pl7ffvutgoKCnO169eqlxMRE53LNmjVd6kycOFHvv/++VqxYoXr16mny5Mnq27evUlJSVKNGjQo7H+BS0tPTlZWV5fa6qampbq8JAACAS/NqkFq9erXLcmJiosLCwpSSkqIuXbo41/v7+ys8PLzEGjk5OVq8eLHeeOMN9ejRQ5K0bNkyNWrUSGvXrlXPnj09dwJAGaWnp6tZsxjl55/1dlcAAADgBl4NUr+Wk5MjSapbt67L+g0bNigsLEzXXHONunbtqr/+9a8KCwuTJKWkpKiwsFDx8fHO9pGRkWrZsqW2bdtWYpAqKChQQUGBczk3N1eSVFhYqMLCQrefV2mKjlWRx4R3HDt2TDabkd2+TFIzN1f/WNIsSQ5JhbLbL46noj+vnEOSXQ6Hg7F6leM9C57AuIInMK5wJco6bmzGGOPhvpSJMUb9+/dXdna2Nm/e7Fy/cuVK1apVS1FRUdq/f7+mT5+u8+fPKyUlRf7+/kpKStKoUaNcgpEkxcfHKzo6WosWLSp2rISEBM2cObPY+qSkJAUGBrr/5AAAAABUCWfPntXQoUOVk5Oj2rVrl9qu0lyRGj9+vL766itt2bLFZf2QIUOcf2/ZsqXatWunqKgoffDBBxo0aFCp9YwxstlsJW6bOnWqJk2a5FzOzc1Vo0aNFB8ff8kXy90KCwuVnJysuLg4+fn5VdhxUfF2797939tVN0lq7ebqb0p60Fnbbi/Ua68la/ToOOXluWNc7ZbURZs2bVLr1u7uO6oS3rPgCYwreALjClei6G61y6kUQWrChAl67733tGnTJjVs2PCSbSMiIhQVFaW9e/dKksLDw3Xu3DllZ2erTp06znbHjh1Tp06dSqzh7+8vf3//Yuv9/Py88sPmreOi4vj4+CgvL08XJ8r0xPe6eO28PD83BSkfSXny8fFhnEIS71nwDMYVPIFxhfIo65jx6vTnxhiNHz9eb7/9tj755BNFR0dfdp/jx4/r0KFDioiIkCS1bdtWfn5+Sk5OdrbJyMjQnj17Sg1SAAAAAHAlvHpF6tFHH1VSUpLeffddBQcHKzMzU5IUEhIiu92u06dPKyEhQffee68iIiJ04MABPfHEEwoNDdXAgQOdbceMGaPJkyerXr16qlu3rqZMmaJWrVo5Z/EDAAAAAHfyapBauHChJCk2NtZlfWJiokaOHKkaNWro66+/1uuvv66TJ08qIiJC3bp108qVKxUcHOxs/9xzz8nX11eDBw9WXl6eunfvriVLlvAZUgAAAAA8wqtB6nITBtrtdq1Zs+aydQICArRgwQItWLDAXV0DAAAAgFJ59RkpAAAAAKiKKsWsfQAqv9TUVI/UDQ0NVePGjT1SGwAAwFMIUgAuI0OSj4YNG+aR6gEBgUpLSyVMAQCAKoUgBeAyTkpySFomKcbNtVOVnz9MWVlZBCkAAFClEKQAlFGMpDbe7gQAAEClwGQTAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKmPwfgdampqR6pGxoayudTAQAAjyBIAfCiDEk+GjZsmEeqBwQEKi0tlTAFAADcjiAFwItOSnJIWqaLH/jrTqnKzx+mrKwsghQAAHA7ghSASiBGUhtvdwIAAKDMmGwCAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwyKtBas6cOWrfvr2Cg4MVFhamAQMGKC0tzaWNMUYJCQmKjIyU3W5XbGysvvnmG5c2BQUFmjBhgkJDQxUUFKR77rlHhw8frshTAQAAAHAV8WqQ2rhxox599FHt2LFDycnJOn/+vOLj43XmzBlnm7lz52r+/Pl68cUXtXPnToWHhysuLk6nTp1ytpk4caJWrVqlFStWaMuWLTp9+rT69u2rCxcueOO0AAAAAFRzvt48+OrVq12WExMTFRYWppSUFHXp0kXGGD3//POaNm2aBg0aJElaunSpGjRooKSkJI0dO1Y5OTlavHix3njjDfXo0UOStGzZMjVq1Ehr165Vz549K/y8AAAAAFRvXg1Sv5aTkyNJqlu3riRp//79yszMVHx8vLONv7+/unbtqm3btmns2LFKSUlRYWGhS5vIyEi1bNlS27ZtKzFIFRQUqKCgwLmcm5srSSosLFRhYaFHzq0kRceqyGPCOxwOh+x2uySHJE98v/+vtt1+sX7Rn+6u736equ2QZJfD4eBnzE14z4InMK7gCYwrXImyjhubMcZ4uC9lYoxR//79lZ2drc2bN0uStm3bps6dO+vIkSOKjIx0tn3ooYd08OBBrVmzRklJSRo1apRLMJKk+Ph4RUdHa9GiRcWOlZCQoJkzZxZbn5SUpMDAQDefGQAAAICq4uzZsxo6dKhycnJUu3btUttVmitS48eP11dffaUtW7YU22az2VyWjTHF1v3apdpMnTpVkyZNci7n5uaqUaNGio+Pv+SL5W6FhYVKTk5WXFyc/Pz8Kuy4qHi7d+9Wly5dJG2S1NrN1d+U9KCztt1eqNdeS9bo0XHKy3PHuHKt716erL1bUhdt2rRJrVu7u/bVifcseALjCp7AuMKVKLpb7XIqRZCaMGGC3nvvPW3atEkNGzZ0rg8PD5ckZWZmKiIiwrn+2LFjatCggbPNuXPnlJ2drTp16ri06dSpU4nH8/f3l7+/f7H1fn5+Xvlh89ZxUXF8fHyUl5eni/O7eOJ7Xbx2Xp6fm4JUyfXdx1O1fSTlycfHh58vN+M9C57AuIInMK5QHmUdM16dtc8Yo/Hjx+vtt9/WJ598oujoaJft0dHRCg8PV3JysnPduXPntHHjRmdIatu2rfz8/FzaZGRkaM+ePaUGKQAAAAC4El69IvXoo48qKSlJ7777roKDg5WZmSlJCgkJkd1ul81m08SJEzV79mw1adJETZo00ezZsxUYGKihQ4c6244ZM0aTJ09WvXr1VLduXU2ZMkWtWrVyzuIHAAAAAO7k1SC1cOFCSVJsbKzL+sTERI0cOVKS9PjjjysvL0/jxo1Tdna2OnTooI8//ljBwcHO9s8995x8fX01ePBg5eXlqXv37lqyZIlq1KhRUacCAAAA4Cri1SBVlgkDbTabEhISlJCQUGqbgIAALViwQAsWLHBj7wBUB6mpqR6rHRoaqsaNG3usPgAAqLwqxWQTAOB+GZJ8NGzYMI8dISAgUGlpqYQpAACuQgQpANXUSV38UN5lkmI8UD9V+fnDlJWVRZACAOAqRJACUM3FSGrj7U4AAIBqxqvTnwMAAABAVUSQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIvKFaT279/v7n4AAAAAQJVRriB14403qlu3blq2bJny8/Pd3ScAAAAAqNTKFaR2796tW2+9VZMnT1Z4eLjGjh2rzz77zN19AwAAAIBKybc8O7Vs2VLz58/X3Llz9f7772vJkiW644471KRJE40ZM0bDhw9X/fr13d1XwKPS09OVlZXlkdqpqakeqQsAAADvKFeQcu7s66uBAweqT58+eumllzR16lRNmTJFU6dO1ZAhQ/TMM88oIiLCXX0FPCY9PV3NmsUoP/+st7sCAACAKuCKZu37/PPPNW7cOEVERGj+/PmaMmWKfvjhB33yySc6cuSI+vfvf8n9N23apH79+ikyMlI2m03vvPOOy/aRI0fKZrO5fN1+++0ubQoKCjRhwgSFhoYqKChI99xzjw4fPnwlp4WrUFZW1n9D1DJJKR74eqoCzwYAAACeVq4rUvPnz1diYqLS0tLUp08fvf766+rTp498fC7msujoaC1atEg33XTTJeucOXNGrVu31qhRo3TvvfeW2KZXr15KTEx0LtesWdNl+8SJE/X+++9rxYoVqlevniZPnqy+ffsqJSVFNWrUKM/p4aoWI6mNB+pyax8AAEB1Uq4gtXDhQo0ePVqjRo1SeHh4iW0aN26sxYsXX7JO79691bt370u28ff3L/UYOTk5Wrx4sd544w316NFDkrRs2TI1atRIa9euVc+ePctwNgAAAABgTbmC1N69ey/bpmbNmhoxYkR5yrvYsGGDwsLCdM0116hr167661//qrCwMElSSkqKCgsLFR8f72wfGRmpli1batu2baUGqYKCAhUUFDiXc3NzJUmFhYUqLCy84j6XVdGxKvKYKJnD4ZDdbpfkkOSp74cn6/9fbbv9Yv2iP91d3/2qam2HJLscDsdV8zPMexY8gXEFT2Bc4UqUddzYjDHGavHExETVqlVLv/nNb1zWv/XWWzp79my5ApTNZtOqVas0YMAA57qVK1eqVq1aioqK0v79+zV9+nSdP39eKSkp8vf3V1JSkkaNGuUSiiQpPj7eeXthSRISEjRz5sxi65OSkhQYGGi57wAAAACqh7Nnz2ro0KHKyclR7dq1S21XritSTz/9tF5++eVi68PCwvTQQw+55UqUJA0ZMsT595YtW6pdu3aKiorSBx98oEGDBpW6nzFGNput1O1Tp07VpEmTnMu5ublq1KiR4uPjL/liuVthYaGSk5MVFxcnPz+/Cjsuitu9e7e6dOkiaZOk1h44wpuSHvRQfdfadnuhXnstWaNHxykvzx3jquL6XnVqS9JuSV20adMmtW7tifqVD+9Z8ATGFTyBcYUrUXS32uWUK0gdPHhQ0dHRxdZHRUUpPT29PCXLJCIiQlFRUc5bC8PDw3Xu3DllZ2erTp06znbHjh1Tp06dSq3j7+8vf3//Yuv9/Py88sPmrePi//j4+CgvL08XJ7L01PfCk/WL187L83NTkCq5vvtU1do+kvLk4+Nz1f388p4FT2BcwRMYVyiPso6Zck1/HhYWpq+++qrY+t27d6tevXrlKVkmx48f16FDh5yfTdW2bVv5+fkpOTnZ2SYjI0N79uy5ZJACAAAAgCtRritSv/3tb/X73/9ewcHB/70dStq4caMee+wx/fa3vy1zndOnT2vfvn3O5f3792vXrl2qW7eu6tatq4SEBN17772KiIjQgQMH9MQTTyg0NFQDBw6UJIWEhGjMmDGaPHmy6tWrp7p162rKlClq1aqVcxY/AAAAAHC3cgWpWbNm6eDBg+revbt8fS+WcDgc+t3vfqfZs2eXuc7nn3+ubt26OZeLnlsaMWKEFi5cqK+//lqvv/66Tp48qYiICHXr1k0rV65UcHCwc5/nnntOvr6+Gjx4sPLy8tS9e3ctWbKEz5ACAAAA4DHlClI1a9bUypUr9dRTT2n37t2y2+1q1aqVoqKiLNWJjY3VpSYNXLNmzWVrBAQEaMGCBVqwYIGlYwMAAABAeZUrSBVp2rSpmjZt6q6+AAAAAECVUK4gdeHCBS1ZskTr1q3TsWPH5HA4XLZ/8sknbukcAAAAAFRG5QpSjz32mJYsWaK7775bLVu2vORnNgEAAABAdVOuILVixQq9+eab6tOnj7v7AwAAAACVXrk+R6pmzZq68cYb3d0XAAAAAKgSyhWkJk+erBdeeOGSM+4BAAAAQHVVrlv7tmzZovXr1+ujjz5SixYt5Ofn57L97bffdkvnAAAAAKAyKleQuuaaazRw4EB39wUAAAAAqoRyBanExER39wMAAAAAqoxyPSMlSefPn9fatWu1aNEinTp1SpJ09OhRnT592m2dAwAAAIDKqFxXpA4ePKhevXopPT1dBQUFiouLU3BwsObOnav8/Hy9/PLL7u4nAAAAAFQa5boi9dhjj6ldu3bKzs6W3W53rh84cKDWrVvnts4BAAAAQGVU7ln7tm7dqpo1a7qsj4qK0pEjR9zSMQAAAACorMp1RcrhcOjChQvF1h8+fFjBwcFX3CkAAAAAqMzKFaTi4uL0/PPPO5dtNptOnz6tGTNmqE+fPu7qGwAAAABUSuW6te+5555Tt27d1Lx5c+Xn52vo0KHau3evQkNDtXz5cnf3EQAAAAAqlXIFqcjISO3atUvLly/XF198IYfDoTFjxuj+++93mXwCAAAAAKqjcgUpSbLb7Ro9erRGjx7tzv4AAAAAQKVXriD1+uuvX3L77373u3J1BgAAAACqgnIFqccee8xlubCwUGfPnlXNmjUVGBhIkAIAAABQrZVr1r7s7GyXr9OnTystLU133HEHk00AAAAAqPbK/YzUrzVp0kRPP/20hg0bpu+++85dZQEX6enpysrKcnvd1NRUt9cEAABA9eW2ICVJNWrU0NGjR91ZEnBKT09Xs2Yxys8/6+2uAE6eCuGhoaFq3LixR2oDAIArV64g9d5777ksG2OUkZGhF198UZ07d3ZLx4Bfy8rK+m+IWiYpxs3VP5Q03c01Ub1lSPLRsGHDPFI9ICBQaWmphCkAACqpcgWpAQMGuCzbbDbVr19fd911l5599ll39Au4hBhJbdxck1v7YNVJSQ55JtinKj9/mLKysghSAABUUuUKUg6Hw939AIAqyhPBHgAAVHblmrUPAAAAAK5m5boiNWnSpDK3nT9/fnkOAQAAAACVVrmC1JdffqkvvvhC58+fV7NmzSRJ33//vWrUqKE2bf7vFhebzeaeXgIAAABAJVKuINWvXz8FBwdr6dKlqlOnjqSLH9I7atQo3XnnnZo8ebJbOwkAAAAAlUm5npF69tlnNWfOHGeIkqQ6depo1qxZzNoHAAAAoNorV5DKzc3VTz/9VGz9sWPHdOrUqSvuFAAAAABUZuUKUgMHDtSoUaP0v//7vzp8+LAOHz6s//3f/9WYMWM0aNAgd/cRAAAAACqVcj0j9fLLL2vKlCkaNmyYCgsLLxby9dWYMWM0b948t3YQAAAAACqbcgWpwMBAvfTSS5o3b55++OEHGWN04403KigoyN39AwAAAIBK54o+kDcjI0MZGRlq2rSpgoKCZIxxV78AAAAAoNIqV5A6fvy4unfvrqZNm6pPnz7KyMiQJD3wwANMfQ4AAACg2itXkPrDH/4gPz8/paenKzAw0Ll+yJAhWr16tds6BwAAAACVUbmekfr444+1Zs0aNWzY0GV9kyZNdPDgQbd0DAAAAAAqq3JdkTpz5ozLlagiWVlZ8vf3v+JOAQAAAEBlVq4g1aVLF73++uvOZZvNJofDoXnz5qlbt25u6xwAAAAAVEblurVv3rx5io2N1eeff65z587p8ccf1zfffKMTJ05o69at7u4jAAAAAFQq5boi1bx5c3311Ve67bbbFBcXpzNnzmjQoEH68ssvdcMNN7i7jwAAAABQqVi+IlVYWKj4+HgtWrRIM2fO9ESfAAAAAKBSs3xFys/PT3v27JHNZvNEfwAAAACg0ivXrX2/+93vtHjxYnf3BQAAAACqhHJNNnHu3Dm9+uqrSk5OVrt27RQUFOSyff78+W7pHAAAAABURpaC1I8//qjrrrtOe/bsUZs2bSRJ33//vUsbbvkDAAAAUN1ZClJNmjRRRkaG1q9fL0kaMmSI/v73v6tBgwYe6RwAAAAAVEaWnpEyxrgsf/TRRzpz5oxbOwQAAAAAlV25Jpso8utgBQAAAABXA0tBymazFXsGimeiAAAAAFxtLD0jZYzRyJEj5e/vL0nKz8/Xww8/XGzWvrffftt9PQQAAACASsZSkBoxYoTL8rBhw9zaGQAAAACoCiwFqcTERE/1AwAAAACqjCuabAIAAAAArkYEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFnk1SG3atEn9+vVTZGSkbDab3nnnHZftxhglJCQoMjJSdrtdsbGx+uabb1zaFBQUaMKECQoNDVVQUJDuueceHT58uALPAgAAAMDVxqtB6syZM2rdurVefPHFErfPnTtX8+fP14svvqidO3cqPDxccXFxOnXqlLPNxIkTtWrVKq1YsUJbtmzR6dOn1bdvX124cKGiTgMAAADAVcbXmwfv3bu3evfuXeI2Y4yef/55TZs2TYMGDZIkLV26VA0aNFBSUpLGjh2rnJwcLV68WG+88YZ69OghSVq2bJkaNWqktWvXqmfPnhV2LgAAAACuHl4NUpeyf/9+ZWZmKj4+3rnO399fXbt21bZt2zR27FilpKSosLDQpU1kZKRatmypbdu2lRqkCgoKVFBQ4FzOzc2VJBUWFqqwsNBDZ1Rc0bEq8phVmcPhkN1ul+SQ5InXzJO1PV3//2rb7RfrF/3p7vruV1Vre7K+Q5JdDoejUr0/8J4FT2BcwRMYV7gSZR03NmOM8XBfysRms2nVqlUaMGCAJGnbtm3q3Lmzjhw5osjISGe7hx56SAcPHtSaNWuUlJSkUaNGuYQiSYqPj1d0dLQWLVpU4rESEhI0c+bMYuuTkpIUGBjovpMCAAAAUKWcPXtWQ4cOVU5OjmrXrl1qu0p7RaqIzWZzWTbGFFv3a5drM3XqVE2aNMm5nJubq0aNGik+Pv6SL5a7FRYWKjk5WXFxcfLz86uw41ZVu3fvVpcuXSRtktTazdXflPSgh2p7ur5rbbu9UK+9lqzRo+OUl+eOcVVxfa86tT1df7ekLtq0aZNat/ZE38uH9yx4AuMKnsC4wpUoulvtciptkAoPD5ckZWZmKiIiwrn+2LFjatCggbPNuXPnlJ2drTp16ri06dSpU6m1/f395e/vX2y9n5+fV37YvHXcqsbHx0d5eXm6OEeKJ14vT9b2dP3itfPy/NwUpEqu7z5VtbYn6/tIypOPj0+lfG/gPQuewLiCJzCuUB5lHTOV9nOkoqOjFR4eruTkZOe6c+fOaePGjc6Q1LZtW/n5+bm0ycjI0J49ey4ZpAAAAADgSnj1itTp06e1b98+5/L+/fu1a9cu1a1bV40bN9bEiRM1e/ZsNWnSRE2aNNHs2bMVGBiooUOHSpJCQkI0ZswYTZ48WfXq1VPdunU1ZcoUtWrVyjmLHwAAAAC4m1eD1Oeff65u3bo5l4ueWxoxYoSWLFmixx9/XHl5eRo3bpyys7PVoUMHffzxxwoODnbu89xzz8nX11eDBw9WXl6eunfvriVLlqhGjRoVfj4AAAAArg5eDVKxsbG61KSBNptNCQkJSkhIKLVNQECAFixYoAULFnighwAAAABQXKV9RgoAAAAAKiuCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLfL3dAQBAyVJTUz1WOzQ0VI0bN/ZYfQAAqjuCFABUOhmSfDRs2DCPHSEgIFBpaamEKQAAyokgBQCVzklJDknLJMV4oH6q8vOHKSsriyAFAEA5EaQAoNKKkdTG250AAAAlYLIJAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIt8vd0BVC/p6enKysrySO3U1FSP1AUAAACsIkjBbdLT09WsWYzy8896uysAAACARxGk4DZZWVn/DVHLJMV44AgfSprugboAAACANQQpeECMpDYeqMutfQAAAKgcmGwCAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEWVOkglJCTIZrO5fIWHhzu3G2OUkJCgyMhI2e12xcbG6ptvvvFijwEAAABcDXy93YHLadGihdauXetcrlGjhvPvc+fO1fz587VkyRI1bdpUs2bNUlxcnNLS0hQcHOyN7gJAlZGammqpvcPhkCTt3r1bPj6l/z9caGioGjdufEV9AwCgsqv0QcrX19flKlQRY4yef/55TZs2TYMGDZIkLV26VA0aNFBSUpLGjh1b0V0FgCoiQ5KPhg0bZmkvu92u5cuXq0uXLsrLyyu1XUBAoNLSUglTAIBqrdIHqb179yoyMlL+/v7q0KGDZs+ereuvv1779+9XZmam4uPjnW39/f3VtWtXbdu27ZJBqqCgQAUFBc7l3NxcSVJhYaEKCws9dzK/UnSsijymJzkcDtntdkkOSZ46J0/Wrx59t9sv1i/609313a+q1vZ0fU/WPinJX9IrkpqVvUd2h6QM2e0bVPqd4WmSHtSxY8cUERFxZd3EVaG6/VuIyoFxhStR1nFjM8YYD/el3D766COdPXtWTZs21U8//aRZs2bpu+++0zfffKO0tDR17txZR44cUWRkpHOfhx56SAcPHtSaNWtKrZuQkKCZM2cWW5+UlKTAwECPnAsAAACAyu/s2bMaOnSocnJyVLt27VLbVeog9WtnzpzRDTfcoMcff1y33367OnfurKNHj7r8r+eDDz6oQ4cOafXq1aXWKemKVKNGjZSVlXXJF8vdCgsLlZycrLi4OPn5+VXYcT1l9+7d6tKli6RNklp74AhvSnrQQ/U9WdvT9V1r2+2Feu21ZI0eHae8PHeMq6r6ulef76n7la9+2cbWbkldtGnTJrVu7f6+Hz58WG3btld+/lm315Yu3paYkrJTDRs29Eh9FFfd/i1E5cC4wpXIzc1VaGjoZYNUpb+175eCgoLUqlUr7d27VwMGDJAkZWZmugSpY8eOqUGDBpes4+/vL39//2Lr/fz8vPLD5q3jupuPj89/n5vwkeSp8/Fk/erV97w8PzcFqZLru09Vre3p+pW375ceWz6S8uTj4+OR97Xs7GxlZx+XtExSjJurpyovb5iys7MVHR3t5tq4nOrybyEqF8YVyqOsY6ZKBamCggKlpqbqzjvvVHR0tMLDw5WcnKxbb71VknTu3Dlt3LhRzzzzjJd7CgDwrBhJbbzdCQDAVaxSB6kpU6aoX79+aty4sY4dO6ZZs2YpNzdXI0aMkM1m08SJEzV79mw1adJETZo00ezZsxUYGKihQ4d6u+sAAAAAqrFKHaQOHz6s++67T1lZWapfv75uv/127dixQ1FRUZKkxx9/XHl5eRo3bpyys7PVoUMHffzxx3yGFAAAAACPqtRBasWKFZfcbrPZlJCQoISEhIrpEAAAAACo9A8CAQAAAACUgiAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARb7e7gAAoPpJTU2tUnUBALCKIAUAcKMMST4aNmyYtzsCAIBHEaQAAG50UpJD0jJJMR6o/6Gk6R6oCwCANQQpAIAHxEhq44G63NoHAKgcmGwCAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIt8vd0BAAAqk9TUVI/VDg0NVePGjT1WHwBQcQhSAABIkjIk+WjYsGEeO0JAQKDS0lIJUwBQDRCkAACQJJ2U5JC0TFKMB+qnKj9/mLKysghSAFANEKQAAHARI6mNtzsBAKjkmGwCAAAAACziihQAABXIU5NZMJEFAFQsghQAABXCs5NZMJEFAFQsghQAABXipDw3mcXFiSw2b96smBhPTJTBFS8A+DWCFAAAFcoTk1kwdTsAVDSCFAAAVd5JMXU7AFQsghQAANUGU7cDQEVh+nMAAAAAsIgrUleh9PR0ZWVlub2up6b0BQAAACobgtRVJj09Xc2axSg//6y3uwIAAABUWQSpq0xWVtZ/Q5QnHkj+UNJ0N9cEAAAAKh+C1FXLEw8kc2sfAFRnnriF2+FwuL0mAFQEghQAALgMz31Old1u1/Lly3X48GFFR0e7vT4AeApBCgAAXMZJee5zqi5e5Tp+/DhBCkCVQpACAABl5Inbwh2Sjri5JgB4Hp8jBQAAAAAWcUUKAACgkvLUZz9KUmhoqBo3buyR2sDVgCAFAABQCXn6sx8DAgKVlpZKmALKiSAFAAC8Li0tTT4+nnnioKpeefHsZz+mKj9/mLKysqrkawNUBgQpAADgRZmSpAcffFB5eXkeOULVv/LiiUk+AFwpghQAAPCiHEm1JL0i9191kbjyAsBTCFIAAKASaCZPXnVJTU31SN2qetsggCtHkAIAANVYhiQfDRs2zCPVq/5tgwDKiyAFAACqsZO6+KG/TNgAwL0IUgAA4CrAhA0A3IsgBQAAcAU89fyVp+oCcA+CVCXkyU8x500ZAAB38ezzVxXBk78XMBEHqrtqE6ReeuklzZs3TxkZGWrRooWef/553Xnnnd7ulmWe/hRzAADgLiflueevJOlDSdM9UFeqiBDIRByo7qpFkFq5cqUmTpyol156SZ07d9aiRYvUu3dvffvtt1Xuh9ezn2IuefZNGQCAq5Gnnr/y5F0kJ+XZEMhEHKXx5J1HklRQUCB/f/8qV1uqelcxq0WQmj9/vsaMGaMHHnhAkvT8889rzZo1WrhwoebMmePl3pVXVXxTBgAAVQuTcFSkirnzqIakC1WwdtW7ilnlg9S5c+eUkpKiP//5zy7r4+PjtW3bthL3KSgoUEFBgXM5JydHknTixAkVFhZ6rrO/UlhYqLNnz+r48ePy8/OTJOXm5iogIEBSiqRcDxw1TZKn6nuytqfrV5++BwQ4dPbsWQUEbJYxPm6v715Vtban61fOvpdtbFXOvnu/tqfrV92+BwTs1dmzzRQQ8KWMOe3W2hfxuld8bUnaKylAKSkpys11f30fHx85HI5StzscF9+vNm/eLB8f6/8WXq5+ee3du1eSQwEBf5LU0O31pS8kLZc0xQP1PVlbkg5LekE//vijgoKCPFC/7E6dOiVJMsZcsp3NXK5FJXf06FFde+212rp1qzp16uRcP3v2bC1dulRpaWnF9klISNDMmTMrspsAAAAAqpBDhw6pYcPSQ2OVvyJVxGazuSwbY4qtKzJ16lRNmjTJuexwOHTixAnVq1ev1H08ITc3V40aNdKhQ4dUu3btCjsuqjfGFTyFsQVPYFzBExhXuBLGGJ06dUqRkZGXbFflg1RoaKhq1KihzMxMl/XHjh1TgwYNStzH39+/2INy11xzjae6eFm1a9fmhxxux7iCpzC24AmMK3gC4wrlFRISctk27niAwqtq1qyptm3bKjk52WV9cnKyy61+AAAAAOAuVf6KlCRNmjRJw4cPV7t27dSxY0f985//VHp6uh5++GFvdw0AAABANVQtgtSQIUN0/Phx/eUvf1FGRoZatmypDz/8UFFRUd7u2iX5+/trxowZHp2PH1cfxhU8hbEFT2BcwRMYV6gIVX7WPgAAAACoaFX+GSkAAAAAqGgEKQAAAACwiCAFAAAAABYRpAAAAADAIoKUF7300kuKjo5WQECA2rZtq82bN3u7S6ikEhISZLPZXL7Cw8Od240xSkhIUGRkpOx2u2JjY/XNN9+41CgoKNCECRMUGhqqoKAg3XPPPTp8+HBFnwq8aNOmTerXr58iIyNls9n0zjvvuGx31zjKzs7W8OHDFRISopCQEA0fPlwnT5708NnBWy43rkaOHFns/ev22293acO4wq/NmTNH7du3V3BwsMLCwjRgwAClpaW5tOE9C95GkPKSlStXauLEiZo2bZq+/PJL3Xnnnerdu7fS09O93TVUUi1atFBGRobz6+uvv3Zumzt3rubPn68XX3xRO3fuVHh4uOLi4nTq1Clnm4kTJ2rVqlVasWKFtmzZotOnT6tv3766cOGCN04HXnDmzBm1bt1aL774Yonb3TWOhg4dql27dmn16tVavXq1du3apeHDh3v8/OAdlxtXktSrVy+X968PP/zQZTvjCr+2ceNGPfroo9qxY4eSk5N1/vx5xcfH68yZM842vGfB6wy84rbbbjMPP/ywy7qbbrrJ/PnPf/ZSj1CZzZgxw7Ru3brEbQ6Hw4SHh5unn37auS4/P9+EhISYl19+2RhjzMmTJ42fn59ZsWKFs82RI0eMj4+PWb16tUf7jspJklm1apVz2V3j6NtvvzWSzI4dO5xttm/fbiSZ7777zsNnBW/79bgyxpgRI0aY/v37l7oP4wplcezYMSPJbNy40RjDexYqB65IecG5c+eUkpKi+Ph4l/Xx8fHatm2bl3qFym7v3r2KjIxUdHS0fvvb3+rHH3+UJO3fv1+ZmZku48nf319du3Z1jqeUlBQVFha6tImMjFTLli0Zc5DkvnG0fft2hYSEqEOHDs42t99+u0JCQhhrV7ENGzYoLCxMTZs21YMPPqhjx445tzGuUBY5OTmSpLp160riPQuVA0HKC7KysnThwgU1aNDAZX2DBg2UmZnppV6hMuvQoYNef/11rVmzRq+88ooyMzPVqVMnHT9+3DlmLjWeMjMzVbNmTdWpU6fUNri6uWscZWZmKiwsrFj9sLAwxtpVqnfv3vrXv/6lTz75RM8++6x27typu+66SwUFBZIYV7g8Y4wmTZqkO+64Qy1btpTEexYqB19vd+BqZrPZXJaNMcXWAdLFX0SKtGrVSh07dtQNN9ygpUuXOh/aLs94Yszh19wxjkpqz1i7eg0ZMsT595YtW6pdu3aKiorSBx98oEGDBpW6H+MKRcaPH6+vvvpKW7ZsKbaN9yx4E1ekvCA0NFQ1atQo9j8dx44dK/Y/K0BJgoKC1KpVK+3du9c5e9+lxlN4eLjOnTun7OzsUtvg6uaucRQeHq6ffvqpWP2ff/6ZsQZJUkREhKKiorR3715JjCtc2oQJE/Tee+9p/fr1atiwoXM971moDAhSXlCzZk21bdtWycnJLuuTk5PVqVMnL/UKVUlBQYFSU1MVERGh6OhohYeHu4ync+fOaePGjc7x1LZtW/n5+bm0ycjI0J49exhzkCS3jaOOHTsqJydHn332mbPNp59+qpycHMYaJEnHjx/XoUOHFBERIYlxhZIZYzR+/Hi9/fbb+uSTTxQdHe2ynfcsVApemeICZsWKFcbPz88sXrzYfPvtt2bixIkmKCjIHDhwwNtdQyU0efJks2HDBvPjjz+aHTt2mL59+5rg4GDneHn66adNSEiIefvtt83XX39t7rvvPhMREWFyc3OdNR5++GHTsGFDs3btWvPFF1+Yu+66y7Ru3dqcP3/eW6eFCnbq1Cnz5Zdfmi+//NJIMvPnzzdffvmlOXjwoDHGfeOoV69e5uabbzbbt28327dvN61atTJ9+/at8PNFxbjUuDp16pSZPHmy2bZtm9m/f79Zv3696dixo7n22msZV7ikRx55xISEhJgNGzaYjIwM59fZs2edbXjPgrcRpLzoH//4h4mKijI1a9Y0bdq0cU7pCfzakCFDTEREhPHz8zORkZFm0KBB5ptvvnFudzgcZsaMGSY8PNz4+/ubLl26mK+//tqlRl5enhk/frypW7eusdvtpm/fviY9Pb2iTwVetH79eiOp2NeIESOMMe4bR8ePHzf333+/CQ4ONsHBweb+++832dnZFXSWqGiXGldnz5418fHxpn79+sbPz880btzYjBgxotiYYVzh10oaU5JMYmKisw3vWfA2mzHGVPRVMAAAAACoynhGCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAqoDrrrtOzz//vLe7USmMHDlSAwYMKNe+Xbp0UVJSkns7VE42m03vvPOOt7vhlJmZqbi4OAUFBemaa67xdndcbNiwQTabTSdPnrxs26+//loNGzbUmTNnPN8xAFc1ghQAXIGRI0fKZrPJZrPJ19dXjRs31iOPPKLs7Gy3Hmfnzp166KGH3FrzUq4krLjLgQMHZLPZtGvXLrfU+89//qPMzEz99re/dUu96ua5555TRkaGdu3ape+//97b3Sm3Vq1a6bbbbtNzzz3n7a4AqOYIUgBwhXr16qWMjAwdOHBAr776qt5//32NGzfOrceoX7++AgMD3VrzavP3v/9do0aNko9P9f2n79y5c+Xe94cfflDbtm3VpEkThYWFubFXZXcl/f+lUaNGaeHChbpw4YJb6gFASarvvyYAUEH8/f0VHh6uhg0bKj4+XkOGDNHHH3/s0iYxMVExMTEKCAjQTTfdpJdeesm5rWPHjvrzn//s0v7nn3+Wn5+f1q9fL6n4rX05OTl66KGHFBYWptq1a+uuu+7S7t27ndtq1KihlJQUSZIxRnXr1lX79u2d+y9fvlwRERHlPudvv/1Wffr0Ua1atdSgQQMNHz5cWVlZzu2xsbH6/e9/r8cff1x169ZVeHi4EhISXGp89913uuOOOxQQEKDmzZtr7dq1Lre7RUdHS5JuvfVW2Ww2xcbGuuz/t7/9TREREapXr54effRRFRYWltrfrKwsrV27Vvfcc4/LepvNpldffVUDBw5UYGCgmjRpovfee8+5fcmSJcVuc3vnnXdks9mcywkJCbrlllv02muvqXHjxqpVq5YeeeQRXbhwQXPnzlV4eLjCwsL017/+tVi/MjIy1Lt3b9ntdkVHR+utt95y2X7kyBENGTJEderUUb169dS/f38dOHDAub3oyuGcOXMUGRmppk2blvoaLFy4UDfccINq1qypZs2a6Y033nBuu+666/Tvf/9br7/+umw2m0aOHFls/6+//lo+Pj7O73N2drZ8fHz0m9/8xtlmzpw56tixo3N548aNuu222+Tv76+IiAj9+c9/1vnz553bY2NjNX78eE2aNEmhoaGKi4uTJH344Ydq2rSp7Ha7unXr5nLOknTw4EH169dPderUUVBQkFq0aKEPP/zQub1nz546fvy4Nm7cWOrrAQBXiiAFAG70448/avXq1fLz83Oue+WVVzRt2jT99a9/VWpqqmbPnq3p06dr6dKlkqT7779fy5cvlzHGuc/KlSvVoEEDde3atdgxjDG6++67lZmZqQ8//FApKSlq06aNunfvrhMnTigkJES33HKLNmzYIEn66quvnH/m5uZKuvjMSUm1yyIjI0Ndu3bVLbfcos8//1yrV6/WTz/9pMGDB7u0W7p0qYKCgvTpp59q7ty5+stf/qLk5GRJksPh0IABAxQYGKhPP/1U//znPzVt2jSX/T/77DNJ0tq1a5WRkaG3337buW39+vX64YcftH79ei1dulRLlizRkiVLSu3zli1bFBgYqJiYmGLbZs6cqcGDB+urr75Snz59dP/99+vEiROWXpMffvhBH330kVavXq3ly5frtdde0913363Dhw9r48aNeuaZZ/Tkk09qx44dLvtNnz5d9957r3bv3q1hw4bpvvvuU2pqqiTp7Nmz6tatm2rVqqVNmzZpy5YtqlWrlnr16uVy5WbdunVKTU1VcnKy/vOf/5TYv1WrVumxxx7T5MmTtWfPHo0dO1ajRo1yBvWdO3eqV69eGjx4sDIyMvTCCy8Uq9GyZUvVq1fPGU42bdqkevXqadOmTc42vxxXR44cUZ8+fdS+fXvt3r1bCxcu1OLFizVr1iyXukuXLpWvr6+2bt2qRYsW6dChQxo0aJD69OmjXbt26YEHHij2Hw2PPvqoCgoKtGnTJn399dd65plnVKtWLef2mjVrqnXr1tq8efOlv3EAcCUMAKDcRowYYWrUqGGCgoJMQECAkWQkmfnz5zvbNGrUyCQlJbns99RTT5mOHTsaY4w5duyY8fX1NZs2bXJu79ixo/njH//oXI6KijLPPfecMcaYdevWmdq1a5v8/HyXmjfccINZtGiRMcaYSZMmmb59+xpjjHn++efN//zP/5g2bdqYDz74wBhjTNOmTc3ChQsveV79+/cvcdv06dNNfHy8y7pDhw4ZSSYtLc0YY0zXrl3NHXfc4dKmffv25k9/+pMxxpiPPvrI+Pr6moyMDOf25ORkI8msWrXKGGPM/v37jSTz5ZdfFutbVFSUOX/+vHPdb37zGzNkyJBSz+e5554z119/fbH1ksyTTz7pXD59+rSx2Wzmo48+MsYYk5iYaEJCQlz2WbVqlfnlP58zZswwgYGBJjc317muZ8+e5rrrrjMXLlxwrmvWrJmZM2eOy7Effvhhl9odOnQwjzzyiDHGmMWLF5tmzZoZh8Ph3F5QUGDsdrtZs2aN87Vo0KCBKSgoKPXcjTGmU6dO5sEHH3RZ95vf/Mb06dPHudy/f38zYsSIS9YZNGiQGT9+vDHGmIkTJ5rJkyeb0NBQ880335jCwkJTq1Yt52v3xBNPFOv/P/7xD1OrVi3n69K1a1dzyy23uBxj6tSpJiYmxmW/P/3pT0aSyc7ONsYY06pVK5OQkHDJvg4cONCMHDnykm0A4EpwRQoArlC3bt20a9cuffrpp5owYYJ69uypCRMmSLp4i96hQ4c0ZswY1apVy/k1a9Ys/fDDD5IuPv8UFxenf/3rX5Kk/fv3a/v27br//vtLPF5KSopOnz6tevXqudTcv3+/s2ZsbKw2b94sh8OhjRs3KjY2VrGxsdq4caMyMzP1/fffl/uKVEpKitavX+9y7JtuukmSnMeXpJtvvtllv4iICB07dkySlJaWpkaNGik8PNy5/bbbbitzH1q0aKEaNWqUWLskeXl5CggIKHHbL/sZFBSk4ODgS9YqyXXXXafg4GDncoMGDdS8eXOX57EaNGhQrO4vb4MrWi66IpWSkqJ9+/YpODjY+TrXrVtX+fn5Lq9zq1atVLNmzUv2LzU1VZ07d3ZZ17lzZ+exyio2NtZ5pXPjxo3q1q2bunTpoo0bN2rnzp3Ky8tzHic1NVUdO3Z0uQ2yc+fOOn36tA4fPuxc165du2J9vf322132+/Xr9Pvf/16zZs1S586dNWPGDOdV11+y2+06e/aspfMDACt8vd0BAKjqgoKCdOONN0q6OKFBt27dNHPmTD311FNyOBySLt7e16FDB5f9fhkE7r//fj322GNasGCBkpKS1KJFC7Vu3brE4zkcDkVERDh/of2loud5unTpolOnTumLL77Q5s2b9dRTT6lRo0aaPXu2brnlFoWFhZV4m1tZOBwO9evXT88880yxbb987uqXtzdKF59HKno9jDEuvyhbdanaJQkNDS11JsVL1fLx8XG55VJSic9ilVTDah9/2U66+Dq3bdvWGbB/qX79+s6/BwUFXbbmL+sWKc/3IDY2Vo899pj27dunPXv26M4779QPP/ygjRs36uTJk2rbtq0zUJZUv+i1/OX6X/f/1693SR544AH17NlTH3zwgT7++GPNmTNHzz77rPM/MCTpxIkTuuGGGyydHwBYwRUpAHCzGTNm6G9/+5uOHj2qBg0a6Nprr9WPP/6oG2+80eWraDIFSRowYIDy8/O1evVqJSUladiwYaXWb9OmjTIzM+Xr61usZmhoqCQ5n5N68cUXZbPZ1Lx5c91555368ssv9Z///KfcV6OKjv/NN9/ouuuuK3b8sv5Sf9NNNyk9PV0//fSTc93OnTtd2hRdZXHHzGu33nqrMjMzLU9LX79+fZ06dcrlM4ncNR27pGLPTO3YscN5da9Nmzbau3evwsLCir3OISEhlo4TExOjLVu2uKzbtm2b5TBd9JzUrFmz1Lp1a9WuXVtdu3bVxo0biz1317x5c23bts0lGG3btk3BwcG69tprSz1G8+bNS3xdfq1Ro0Z6+OGH9fbbb2vy5Ml65ZVXXLbv2bNHt956q6XzAwArCFIA4GaxsbFq0aKFZs+eLenirG5z5szRCy+8oO+//15ff/21EhMTNX/+fOc+QUFB6t+/v6ZPn67U1FQNHTq01Po9evRQx44dNWDAAK1Zs0YHDhzQtm3b9OSTT+rzzz936ceyZcvUtWtX2Ww21alTR82bN9fKlSuLzYBXkpycHO3atcvlKz09XY8++qhOnDih++67T5999pl+/PFHffzxxxo9enSZQ09cXJxuuOEGjRgxQl999ZW2bt3qnGyi6GpFWFiY7Ha7czKLnJycMtUuya233qr69etr69atlvbr0KGDAgMD9cQTT2jfvn1KSkq65KQWVr311lt67bXX9P3332vGjBn67LPPNH78eEkXr1KGhoaqf//+2rx5s/bv36+NGzfqsccec7k1riz++Mc/asmSJXr55Ze1d+9ezZ8/X2+//bamTJliqY7NZlOXLl20bNky5xi6+eabde7cOa1bt85lXI0bN06HDh3ShAkT9N133+ndd9/VjBkzNGnSpEtOQf/www/rhx9+0KRJk5SWllbiaz5x4kStWbNG+/fv1xdffKFPPvnEJRQeOHBAR44cUY8ePSydHwBYQZACAA+YNGmSXnnlFR06dEgPPPCAXn31VS1ZskStWrVS165dtWTJEpcrUtLFX5x3796tO++8U40bNy61ts1m04cffqguXbpo9OjRatq0qX7729/qwIEDatCggbNdt27ddOHCBZdfbrt27aoLFy6U6YrUhg0bdOutt7p8/b//9/8UGRmprVu36sKFC+rZs6datmypxx57TCEhIWX+jKYaNWronXfe0enTp9W+fXs98MADevLJJyXJ+SyTr6+v/v73v2vRokWKjIxU//79y1S7tOONHj26xNvkLqVu3bpatmyZPvzwQ7Vq1UrLly8vNo37lZg5c6ZWrFihm2++WUuXLtW//vUvNW/eXJIUGBioTZs2qXHjxho0aJBiYmI0evRo5eXlqXbt2paOM2DAAL3wwguaN2+eWrRooUWLFikxMbFMgfrXfj2ubDab7rzzTknSHXfc4Wx37bXX6sMPP9Rnn32m1q1b6+GHH9aYMWOc3+fSNG7cWP/+97/1/vvvq3Xr1nr55Zed/ylR5MKFC3r00UcVExOjXr16qVmzZi4fKbB8+XLFx8crKirK8vkBQFnZTFluRgYAwMO2bt2qO+64Q/v27fPIsy0//fSTWrRooZSUFH7BrsYKCgrUpEkTLV++vNgEGwDgTgQpAIBXrFq1SrVq1VKTJk20b98+PfbYY6pTp06xZ3nc6d1331XdunWdV1BQ/Xz//fdav369xo4d6+2uAKjmCFIAAK94/fXX9dRTT+nQoUMKDQ1Vjx499Oyzz6pevXre7hoAAJdFkAIAAAAAi5hsAgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGDR/wfueognFjfu8gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plotting the histogram of review lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(review_lengths, bins=30, color='blue', edgecolor='black')\n",
    "plt.title('Histogram of Review Lengths')\n",
    "plt.xlabel('Review Length (number of words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d55364-6548-49b8-9ec0-87143cff3659",
   "metadata": {},
   "source": [
    "#### vii. To represent each text (= data point), there are many ways. In NLP/Deep Learning terminology, this task is called tokenization. It is common to represent text using popularity/ rank of words in text. The most common word in the text will be represented as 1, the second most common word will be represented as 2, etc. Tokenize each text document using this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd5ea2-0a08-427a-a282-171bd754c899",
   "metadata": {},
   "source": [
    "I tried to code it without using the Kera first\n",
    "#### Method 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210eb5e6-2c8b-4ac1-8646-b8ee50fd7a00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: films adapted from comic books have had plenty of success  whether theyre about superheroes  batman  superman  spawn   or geared toward kids  casper  or the arthouse crowd  ghost world   but theres never really been a comic book like from hell before  \n",
      "for starters  it was created by alan moore  and eddie campbell   who brought the medium to a whole new level in the mid s with a part series called the watchmen  \n",
      "to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd  \n",
      "the book  or  graphic novel   if you will  is over  pages long and includes nearly  more that consist of nothing but footnotes  \n",
      "in other words  dont dismiss this film because of its source  \n",
      "if you can get past the whole comic book thing  you might find another stumbling block in from hells directors  albert and allen hughes  \n",
      "getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in  well  anything  but riddle me this  who better to direct a film thats set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society  \n",
      "the ghetto in question is  of course  whitechapel in  londons east end  \n",
      "its a filthy  sooty place where the whores  called  unfortunates   are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision  \n",
      "when the first stiff turns up  copper peter godley  robbie coltrane  the world is not enough  calls in inspector frederick abberline  johnny depp  blow  to crack the case  \n",
      "abberline  a widower  has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium  \n",
      "upon arriving in whitechapel  he befriends an unfortunate named mary kelly  heather graham  say it isnt so  and proceeds to investigate the horribly gruesome crimes that even the police surgeon cant stomach  \n",
      "i dont think anyone needs to be briefed on jack the ripper  so i wont go into the particulars here  other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay  \n",
      "in the comic  they dont bother cloaking the identity of the ripper  but screenwriters terry hayes  vertical limit  and rafael yglesias  les mis  rables  do a good job of keeping him hidden from viewers until the very end  \n",
      "its funny to watch the locals blindly point the finger of blame at jews and indians because  after all  an englishman could never be capable of committing such ghastly acts  \n",
      "and from hells ending had me whistling the stonecutters song from the simpsons for days   who holds back the electric carwho made steve guttenberg a star     \n",
      "dont worry  itll all make sense when you see it  \n",
      "now onto from hells appearance  its certainly dark and bleak enough  and its surprising to see how much more it looks like a tim burton film than planet of the apes did  at times  it seems like sleepy hollow    \n",
      "the print i saw wasnt completely finished  both color and music had not been finalized  so no comments about marilyn manson   but cinematographer peter deming  dont say a word  ably captures the dreariness of victorianera london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks  even though the violence in the film pales in comparison to that in the blackandwhite comic  \n",
      "oscar winner martin childs  shakespeare in love  production design turns the original prague surroundings into one creepy place  \n",
      "even the acting in from hell is solid  with the dreamy depp turning in a typically strong performance and deftly handling a british accent  \n",
      "ians holm  joe goulds secret  and richardson   dalmatians  log in great supporting roles  but the big surprise here is graham  \n",
      "i cringed the first time she opened her mouth  imagining her attempt at an irish accent  but it actually wasnt half bad  \n",
      "the film  however  is all good  \n",
      "    r for strong violencegore  sexuality  language and drug content \n",
      "\n",
      "Tokenized Text: [67, 2742, 29, 355, 1632, 32, 90, 1046, 4, 627, 628, 321, 40, 7654, 703, 4780, 1755, 47, 7253, 1327, 396, 4946, 47, 1, 10835, 1777, 1880, 148, 16, 140, 108, 89, 68, 2, 355, 405, 39, 29, 502, 142, 12, 9969, 9, 31, 853, 21, 1351, 1801, 3, 944, 1508, 28, 877, 1, 3949, 5, 2, 283, 113, 540, 7, 1, 7254, 127, 11, 2, 202, 262, 381, 1, 27391, 5, 179, 1801, 3, 1508, 1881, 27392, 1, 845, 4, 495, 1, 13233, 69, 22, 39, 820, 324, 1231, 6, 2138, 5, 175, 2, 94, 1232, 1, 405, 47, 1546, 521, 51, 35, 65, 6, 111, 4947, 196, 3, 1233, 440, 42, 8, 9258, 4, 181, 16, 20956, 7, 73, 637, 119, 6910, 14, 15, 84, 4, 23, 2109, 51, 35, 64, 74, 403, 1, 283, 355, 405, 180, 35, 228, 188, 124, 6592, 2807, 7, 29, 11936, 878, 2857, 3, 1029, 2590, 333, 1, 2590, 620, 5, 1964, 14, 136, 178, 10, 2969, 10, 1089, 8116, 466, 7, 83, 232, 16, 27393, 103, 14, 28, 153, 5, 1964, 2, 15, 185, 259, 7, 1, 4626, 3, 629, 89, 995, 1047, 689, 58, 1, 1608, 7655, 384, 1261, 1019, 911, 1, 4626, 7, 486, 6, 4, 218, 20957, 7, 14987, 3313, 133, 23, 2, 13234, 27394, 235, 96, 1, 20958, 381, 20959, 20, 2138, 5, 74, 2, 94, 3240, 40, 14, 1147, 6911, 28, 33, 68, 13235, 121, 45, 3556, 11, 20960, 8117, 43, 1, 80, 4948, 336, 46, 13236, 524, 27395, 3842, 13237, 1, 148, 6, 25, 154, 1328, 7, 3087, 10836, 20961, 1778, 2858, 2459, 5, 3241, 1, 318, 20961, 2, 10837, 33, 13238, 1060, 17, 9259, 364, 5, 17387, 11, 11937, 3557, 4, 20962, 3, 20963, 436, 5988, 7, 20957, 17, 4068, 24, 2139, 382, 1008, 1547, 3738, 2808, 179, 9, 162, 38, 3, 3950, 5, 2498, 1, 2859, 4627, 3739, 8, 56, 1, 555, 8118, 220, 4781, 18, 119, 172, 325, 612, 5, 22, 27396, 19, 495, 1, 13233, 38, 18, 501, 129, 55, 1, 11938, 125, 73, 58, 5, 179, 1801, 3, 1508, 32, 2, 1172, 3, 222, 2591, 40, 157, 1, 1675, 4, 1, 504, 3, 1, 1030, 17, 3242, 5, 20964, 7, 1, 355, 36, 119, 2544, 27397, 1, 1675, 4, 1, 13233, 16, 1911, 2337, 10838, 11939, 8621, 3, 11940, 17388, 4195, 14988, 20965, 76, 2, 61, 254, 4, 1525, 54, 1709, 29, 732, 308, 1, 78, 133, 23, 176, 5, 237, 1, 6285, 13239, 212, 1, 4782, 4, 1526, 30, 4628, 3, 7255, 84, 81, 37, 24, 14989, 104, 108, 22, 1940, 4, 7256, 118, 9260, 1802, 3, 29, 11936, 314, 90, 103, 27398, 1, 27399, 1048, 29, 1, 7656, 12, 394, 28, 1581, 135, 1, 6593, 27400, 137, 903, 14990, 2, 209, 119, 3391, 4338, 37, 86, 253, 43, 35, 82, 9, 144, 1140, 29, 11936, 1090, 23, 376, 432, 3, 2499, 154, 3, 23, 1329, 5, 82, 93, 70, 42, 9, 299, 39, 2, 885, 2195, 15, 58, 565, 4, 1, 1527, 192, 30, 245, 9, 136, 39, 3025, 1882, 1, 4196, 18, 526, 457, 300, 3392, 157, 1548, 3, 285, 90, 25, 68, 20966, 38, 59, 2368, 40, 6912, 9261, 16, 1986, 524, 27401, 119, 179, 2, 638, 8119, 3161, 1, 20967, 4, 27402, 2110, 3, 2338, 86, 1, 3088, 982, 117, 3740, 103, 4, 1, 1582, 1912, 7, 2970, 5989, 56, 151, 1, 448, 7, 1, 15, 13240, 7, 1822, 5, 8, 7, 1, 7657, 355, 654, 1883, 837, 4486, 1884, 7, 126, 460, 1528, 336, 1, 203, 27403, 5990, 55, 26, 1441, 235, 56, 1, 206, 7, 29, 502, 6, 815, 11, 1, 9262, 2858, 1676, 7, 2, 4197, 507, 158, 3, 10839, 4629, 2, 959, 1396, 27404, 5991, 467, 17389, 789, 3, 4339, 5329, 14991, 7, 123, 490, 480, 16, 1, 145, 655, 125, 6, 2808, 18, 17390, 1, 80, 63, 53, 3026, 34, 1478, 13241, 34, 505, 30, 24, 2809, 1396, 16, 9, 173, 457, 342, 106, 1, 15, 143, 6, 37, 61, 886, 12, 507, 17391, 3393, 1107, 3, 923, 1710]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Step 1: Create a frequency dictionary\n",
    "word_counts = Counter(word for text in all_texts for word in text.split())\n",
    "# Step 2: Sort words by frequency (most common first)\n",
    "sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "# Step 3: Create a dictionary to map words to their ranks\n",
    "word_ranks = {word: rank for rank, word in enumerate(sorted_words, start=1)}\n",
    "\n",
    "# Step 4: Tokenize each text document using the word ranks\n",
    "tokenized_texts = [[word_ranks[word] for word in text.split()] for text in train_texts]\n",
    "\n",
    "# Example of how the tokenized data looks for the first document\n",
    "print(f\"Original Text: {train_texts[0]}\")\n",
    "print(f\"Tokenized Text: {tokenized_texts[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09550e-5cd1-4d26-b4d2-339b83e08fa5",
   "metadata": {},
   "source": [
    "#### Method 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0528dd-e298-48dd-aba0-20133d4df048",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 2745, 29, 355, 1634, 32, 90, 1047, 4, 627, 628, 321, 40, 7660, 704, 4783, 1755, 47, 7257, 1328, 396, 4949, 47, 1, 10837, 1777, 1778, 148, 16, 140, 108, 89, 68, 2, 355, 405, 39, 29, 503, 142, 12, 9974, 9, 31, 853, 21, 1352, 1802, 3, 944, 1512, 28, 879, 1, 3951, 5, 2, 282, 113, 540, 7, 1, 7258, 126, 11, 2, 202, 262, 381, 1, 27315, 5, 180, 1802, 3, 1512, 1882, 27316, 1, 845, 4, 495, 1, 13221, 69, 22, 39, 820, 325, 1231, 6, 2142, 5, 175, 2, 94, 1232, 1, 405, 47, 1549, 523, 51, 35, 65, 6, 111, 4950, 196, 3, 1233, 440, 42, 8, 9266, 4, 181, 16, 20915, 7, 73, 637, 119, 6914, 14, 15, 84, 4, 23, 2110, 51, 35, 64, 74, 403, 1, 282, 355, 405, 179, 35, 228, 188, 124, 6595, 2808, 7, 29, 11930, 880, 2858, 3, 1029, 2592, 333, 1, 2592, 620, 5, 1967, 14, 136, 178, 10, 2970, 10, 1089, 8121, 466, 7, 83, 232, 16, 27317, 103, 14, 28, 153, 5, 1967, 2, 15, 185, 260, 7, 1, 4628, 3, 629, 89, 996, 1036, 690, 58, 1, 1609, 7661, 384, 1261, 1009, 911, 1, 4628, 7, 486, 6, 4, 218, 20916, 7, 14973, 3316, 133, 23, 2, 13222, 27318, 235, 96, 1, 20917, 381, 20918, 20, 2142, 5, 74, 2, 94, 3240, 40, 14, 1147, 6915, 28, 33, 68, 13223, 121, 45, 3557, 11, 20919, 8122, 43, 1, 80, 4951, 336, 46, 13224, 524, 27319, 3844, 13225, 1, 148, 6, 25, 154, 1329, 7, 3089, 10838, 20920, 1779, 2859, 2462, 5, 3241, 1, 318, 20920, 2, 10839, 33, 13226, 1061, 17, 9267, 364, 5, 17365, 11, 11931, 3558, 4, 20921, 3, 20922, 436, 5997, 7, 20916, 17, 4070, 24, 2143, 382, 1010, 1550, 3738, 2809, 180, 9, 162, 38, 3, 3952, 5, 2500, 1, 2860, 4629, 3739, 8, 56, 1, 555, 8123, 219, 4784, 18, 119, 172, 326, 612, 5, 22, 27320, 19, 495, 1, 13221, 38, 18, 501, 129, 55, 1, 11932, 125, 73, 58, 5, 180, 1802, 3, 1512, 32, 2, 1177, 3, 222, 2593, 40, 157, 1, 1675, 4, 1, 505, 3, 1, 1030, 17, 3242, 5, 20923, 7, 1, 355, 36, 119, 2549, 27321, 1, 1675, 4, 1, 13221, 16, 1913, 2340, 10840, 11933, 8626, 3, 11934, 17366, 4197, 14974, 20924, 76, 2, 61, 254, 4, 1529, 54, 1711, 29, 732, 308, 1, 78, 133, 23, 176, 5, 237, 1, 6290, 13227, 212, 1, 4785, 4, 1530, 30, 4630, 3, 7259, 84, 81, 37, 24, 14975, 104, 108, 22, 1941, 4, 7260, 118, 9268, 1803, 3, 29, 11930, 314, 90, 103, 27322, 1, 27323, 1048, 29, 1, 7662, 12, 392, 28, 1582, 135, 1, 6596, 27324, 137, 903, 14976, 2, 209, 119, 3393, 4340, 37, 86, 253, 43, 35, 82, 9, 144, 1140, 29, 11930, 1090, 23, 377, 432, 3, 2501, 154, 3, 23, 1330, 5, 82, 93, 70, 42, 9, 299, 39, 2, 886, 2198, 15, 58, 565, 4, 1, 1531, 192, 30, 245, 9, 136, 39, 3026, 1883, 1, 4198, 18, 526, 457, 300, 3394, 157, 1551, 3, 285, 90, 25, 68, 20925, 38, 59, 2371, 40, 6916, 9269, 16, 1988, 524, 27325, 119, 180, 2, 638, 8124, 3161, 1, 20926, 4, 27326, 2111, 3, 2341, 86, 1, 3090, 982, 117, 3740, 103, 4, 1, 1583, 1914, 7, 2971, 5998, 56, 151, 1, 448, 7, 1, 15, 13228, 7, 1823, 5, 8, 7, 1, 7663, 355, 655, 1884, 837, 4488, 1885, 7, 127, 460, 1532, 336, 1, 203, 27327, 5999, 55, 26, 1443, 235, 56, 1, 206, 7, 29, 503, 6, 816, 11, 1, 9270, 2859, 1676, 7, 2, 4199, 509, 158, 3, 10841, 4631, 2, 960, 1397, 27328, 6000, 467, 17367, 789, 3, 4341, 5336, 14977, 7, 123, 490, 481, 16, 1, 145, 656, 125, 6, 2809, 18, 17368, 1, 80, 63, 53, 3027, 34, 1481, 13229, 34, 506, 30, 24, 2810, 1397, 16, 9, 174, 457, 342, 106, 1, 15, 143, 6, 37, 61, 887, 12, 509, 17369, 3395, 1108, 3, 923, 1712]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = Tokenizer()  # By default, it sorts words by frequency\n",
    "\n",
    "# Fit the tokenizer on the texts\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "\n",
    "print(sequences[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8789aa-fd15-4e6d-b270-fa8dcfecc7fc",
   "metadata": {},
   "source": [
    "#### viii. Select a review length L that 70% of the reviews have a length below it. If you feel more adventurous, set the threshold to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74e3184-1cd3-4840-b097-79c8efa2ce01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 70th percentile of review lengths: 737 words\n",
      "The 90th percentile of review lengths: 993 words\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming review_lengths is already defined from previous steps\n",
    "review_lengths = np.array([len(text.split()) for text in all_texts])\n",
    "\n",
    "# Calculate the 70th percentile\n",
    "L_70 = np.percentile(review_lengths, 70)\n",
    "print(f\"The 70th percentile of review lengths: {L_70:.0f} words\")\n",
    "\n",
    "# Calculate the 90th percentile if feeling adventurous\n",
    "L_90 = np.percentile(review_lengths, 90)\n",
    "print(f\"The 90th percentile of review lengths: {L_90:.0f} words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273be71-a3e7-49c0-85fa-8628aa3930ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ix.Truncate reviews longer than L words and zero-pad reviews shorter than L so that all texts (= data points) are of length L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af31572-7fbc-48bc-b3bf-93f96baee36e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "films adapted from comic books have had plenty of success whether theyre about superheroes batman superman spawn or geared toward kids casper or the arthouse crowd ghost world but theres never really been a comic book like from hell before for starters it was created by alan moore and eddie campbell who brought the medium to a whole new level in the mid s with a part series called the watchmen to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd the book or graphic novel if you will is over pages long and includes nearly more that consist of nothing but footnotes in other words dont dismiss this film because of its source if you can get past the whole comic book thing you might find another stumbling block in from hells directors albert and allen hughes getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in well anything but riddle me this who better to direct a film thats set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society the ghetto in question is of course whitechapel in londons east end its a filthy sooty place where the whores called unfortunates are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision when the first stiff turns up copper peter godley robbie coltrane the world is not enough calls in inspector frederick abberline johnny depp blow to crack the case abberline a widower has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium upon arriving in whitechapel he befriends an unfortunate named mary kelly heather graham say it isnt so and proceeds to investigate the horribly gruesome crimes that even the police surgeon cant stomach i dont think anyone needs to be briefed on jack the ripper so i wont go into the particulars here other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay in the comic they dont bother cloaking the identity of the ripper but screenwriters terry hayes vertical limit and rafael yglesias les mis rables do a good job of keeping him hidden from viewers until the very end its funny to watch the locals blindly point the finger of blame at jews and indians because after all an englishman could never be capable of committing such ghastly acts and from hells ending had me whistling the stonecutters song from the simpsons for days who holds back the electric carwho made steve guttenberg a star dont worry itll all make sense when you see it now onto from hells appearance its certainly dark and bleak enough and its surprising to see how much more it looks like a tim burton film than planet of the apes did at times it seems like sleepy hollow the print i saw wasnt completely finished both color and music had not been finalized so no comments about marilyn manson but cinematographer peter deming dont say a word ably captures the dreariness of victorianera london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks even though the violence in the film pales in comparison to that in the blackandwhite comic oscar winner martin childs shakespeare in love production design turns the original prague surroundings into one creepy place even the acting in from hell is solid with the dreamy depp turning in a typically strong performance and deftly handling a british accent ians holm joe goulds secret and richardson dalmatians log in great supporting roles but the big surprise here is graham i cringed the first time she opened her mouth imagining her attempt at an irish accent but it actually wasnt half bad the film however is all good r for strong violencegore sexuality language and drug content 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to truncate or pad reviews\n",
    "def truncate_or_pad(texts, max_length):\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        if len(words) > max_length:\n",
    "            # Truncate the review\n",
    "            processed_text = words[:max_length]\n",
    "        else:\n",
    "            # Pad the review with '0' (you can choose any other padding token)\n",
    "            processed_text = words + ['0'] * (max_length - len(words))\n",
    "        \n",
    "        processed_texts.append(processed_text)\n",
    "    \n",
    "    return processed_texts\n",
    "\n",
    "# Apply the function to train_texts\n",
    "processed_texts = truncate_or_pad(train_texts, 737)\n",
    "\n",
    "# Convert processed texts back to strings if necessary\n",
    "processed_texts = [' '.join(text) for text in processed_texts]\n",
    "\n",
    "# Optionally convert to a numpy array\n",
    "processed_texts_array = np.array(processed_texts)\n",
    "\n",
    "# Show the first few processed texts\n",
    "print(processed_texts_array[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d89bc8-252d-418e-b98b-6c6ba006c131",
   "metadata": {},
   "source": [
    "### c) Word Embeddings\n",
    "#### i. One can use tokenized text as inputs to a deep neural network. However, a re- cent breakthrough in NLP suggests that more sophisticated representations of text yield better results. These sophisticated representations are called word embeddings . â€œWord embedding is a term used for representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.â€ 4 . Most deep learning modules (including Keras) provide a convenient way to convert positive integer rep- resentations of words into a word embedding by an â€œEmbedding layer.â€ The layer accepts arguments that define the mapping of words into embeddings, including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value). The layer also allows you to specify the dimension for each word vector, called the â€œoutput dimension.â€ We would like to use a word embedding layer for this project. Assume that we are inter- ested in the top 5,000 words. This means that in each integer sequence that represents each document, we set to zero those integers that represent words that are not among the top 5,000 words in the document.(This is done by setting an argument in the embedding layer provided by Keras. Example: model.add(Embedding(top_words, 32, input_length=max_words)), where top_words=5,000 and max_words=L) 5 If you feel more adventurous, use all the words that appear in this corpus. Choose the length of the embedding vector for each word to be 32. Hence, each document is represented as a 32 Ã— L matrix. ii. Flatten the matrix of each document to a vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecfce1b-fd04-4b27-b9b0-597f0cd03481",
   "metadata": {},
   "source": [
    "When you use a 32-dimensional vector to represent each word in your corpus, as is typical with word embeddings, each dimension of the vector encodes some aspect of the word's semantic and syntactic properties. However, the meanings of individual dimensions in such vectors are not typically interpretable in isolation. The embeddings are learned in such a way that the vector as a whole represents the word's usage in language, based on how it appears with other words across a large text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac3e6ae2-127f-4db3-89c6-fe78fc13158c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated labels and counts: {0: 700, 1: 700}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Convert -1 to 0\n",
    "train_labels[train_labels == -1] = 0\n",
    "\n",
    "# check the updated unique values and their counts\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"Updated labels and counts:\", dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf1d6f5b-0ebe-4163-962a-ce9e545d7dec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca77c21-61c5-4b97-9002-b7ad595d0282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62bcf2be-465b-4ab1-b4f6-e9e881856032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 5000  # Only the top 5000 words will be considered\n",
    "max_length = 737   # This will be the length each input sequence is padded to\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)  # Initializes tokenizer\n",
    "tokenizer.fit_on_texts(processed_texts) \n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(processed_texts)  # Converts texts to sequences of integers\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')  # Ensures uniform input length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "050a72f7-9e87-4623-b769-2ca5044a9e53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  69, 3384,   30, ...,    0,    0,    0],\n",
       "       [ 151,  136,    4, ...,    0,    0,    0],\n",
       "       [ 707,  284, 3829, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 682,  589, 4236, ...,    0,    0,    0],\n",
       "       [  83, 1719,    3, ...,    0,    0,    0],\n",
       "       [1159,   19,   51, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc2c41-156e-4929-8f37-37c1ec101848",
   "metadata": {},
   "source": [
    "### (d) Multi-Layer Perceptron\n",
    "#### i.Train a MLP with three (dense) hidden layers each of which has 50 ReLUs and one output layer with a single sigmoid neuron. Use a dropout rate of 20% for the first layer and 50% for the other layers. Use ADAM optimizer and binary cross entropy loss (which is equivalent to having a softmax in the output). To avoid overfitting, just set the number of epochs as 2. Use a batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78f8c166-5f87-4f5f-a0c5-575ef44621f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4832 - loss: 0.7049\n",
      "Epoch 2/2\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5486 - loss: 0.6803\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "\n",
    "\n",
    "# Assuming the input parameters are set\n",
    "vocab_size = 5000  # Top 5000 words\n",
    "embedding_dim = 32  # Each word will be represented by a 32-dimensional vector\n",
    "max_length = 737    # Maximum length of the sequences\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential([\n",
    "    # Embedding layer\n",
    "    Embedding(input_dim=vocab_size+1, output_dim=embedding_dim, input_length=max_length, trainable=True),\n",
    "    \n",
    "    # Flatten the embeddings output to feed into a dense layer\n",
    "    Flatten(),\n",
    "    \n",
    "    # First hidden layer with dropout at 20%\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Second hidden layer with dropout at 50%\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Third hidden layer with dropout at 50%\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Output layer with a single sigmoid unit for binary classification\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Fit the model on the training data\n",
    "history = model.fit(padded_sequences, train_labels, epochs=2, batch_size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd4ecf13-50d9-4e48-976b-e4dab6928857",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 737)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81fdfab2-ddfe-48bc-a089-f9d70d269f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8183 - loss: 0.5829 \n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8001 - loss: 0.6119\n"
     ]
    }
   ],
   "source": [
    "# process the text data\n",
    "\n",
    "processed_texts_test = truncate_or_pad(test_texts, 737)\n",
    "# Convert processed texts back to strings if necessary\n",
    "processed_texts_test = [' '.join(text) for text in processed_texts_test]\n",
    "\n",
    "vocab_size = 5000  # Only the top 5000 words will be considered\n",
    "max_length = 737   \n",
    "\n",
    "tokenizer_t = Tokenizer(num_words=vocab_size)  # Initializes tokenizer\n",
    "tokenizer_t.fit_on_texts(processed_texts_test)  \n",
    "\n",
    "sequences_t = tokenizer_t.texts_to_sequences(processed_texts_test) \n",
    "padded_sequences_test = pad_sequences(sequences_t, maxlen=max_length, padding='post', truncating='post')  # Ensures uniform input length\n",
    "\n",
    "\n",
    "\n",
    "# Convert -1 to 0\n",
    "test_labels[test_labels == -1] = 0\n",
    "\n",
    "\n",
    "train_loss, train_accuracy = model.evaluate(padded_sequences, train_labels)\n",
    "test_loss, test_accuracy = model.evaluate(padded_sequences_test, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb7d50-23af-49b9-bfd5-33f11e596ccb",
   "metadata": {},
   "source": [
    "### (e) One-Dimensional Convolutional Neural Network : Although CNNs are mainly used for image data, they can also be applied to text data, as text also has adjacency information. Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes respectively\n",
    "#### i. After the embedding layer, insert a Conv1D layer. This convolutional layer has 32 feature maps , and each of the 32 kernels has size 3, i.e. reads embedded word representations 3 vector elements of the word embedding at a time. The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 that halves the size of the feature maps from the convolutional layer. The rest of the network is the same as the neural network above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8234dc5f-5e37-4a2f-b702-3893564d4b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24bfce12-c0d1-4bde-932c-a9deada22ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 5001  # Top 5000 words plus padding index\n",
    "embedding_dim = 32\n",
    "max_length = 737\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),  # 32 feature maps and kernel size of 3\n",
    "    MaxPooling1D(pool_size=2), \n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1286e966-fd5f-4e3e-9171-996c70c902ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4877 - loss: 0.7034\n",
      "Epoch 2/2\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5201 - loss: 0.6915\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    padded_sequences,  \n",
    "    train_labels, \n",
    "    epochs=2,\n",
    "    batch_size=10,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "853eedfa-668d-49fb-8063-4c63a879fc19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5987 - loss: 0.6828\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5243 - loss: 0.6958\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# process the text data\n",
    "processed_texts_test = truncate_or_pad(test_texts, 737)\n",
    "# Convert processed texts back to strings if necessary\n",
    "processed_texts_test = [' '.join(text) for text in processed_texts_test]\n",
    "\n",
    "vocab_size = 5000  # Only the top 5000 words will be considered\n",
    "max_length = 737   \n",
    "\n",
    "tokenizer_t = Tokenizer(num_words=vocab_size)  \n",
    "tokenizer_t.fit_on_texts(processed_texts_test)\n",
    "sequences_t = tokenizer_t.texts_to_sequences(processed_texts_test) \n",
    "padded_sequences_test = pad_sequences(sequences_t, maxlen=max_length, padding='post', truncating='post')  # Ensures uniform input length\n",
    "\n",
    "\n",
    "\n",
    "# Convert -1 to 0\n",
    "test_labels[test_labels == -1] = 0\n",
    "\n",
    "\n",
    "train_loss, train_accuracy = model.evaluate(padded_sequences, train_labels)\n",
    "test_loss, test_accuracy = model.evaluate(padded_sequences_test, test_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc64d61-4af3-4bf7-8f56-5388989dc3e2",
   "metadata": {},
   "source": [
    "### (f) Long Short-Term Memory Recurrent Neural Network\n",
    "#### i. Each word is represented to LSTM as a vector of 32 elements and the LSTM is followed by a dense layer of 256 ReLUs. Use a dropout rate of 0.2 for both LSTM and the dense layer. Train the model using 10-50 epochs and batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acc907ed-301b-4355-b791-7481ca03df4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.4803 - loss: 0.6941\n",
      "Epoch 2/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 60ms/step - accuracy: 0.4904 - loss: 0.6942\n",
      "Epoch 3/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - accuracy: 0.4925 - loss: 0.6938\n",
      "Epoch 4/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - accuracy: 0.4923 - loss: 0.6937\n",
      "Epoch 5/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.5090 - loss: 0.6933\n",
      "Epoch 6/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - accuracy: 0.5047 - loss: 0.6934\n",
      "Epoch 7/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - accuracy: 0.4856 - loss: 0.6940\n",
      "Epoch 8/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 63ms/step - accuracy: 0.4998 - loss: 0.6931\n",
      "Epoch 9/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - accuracy: 0.4783 - loss: 0.6932\n",
      "Epoch 10/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 60ms/step - accuracy: 0.4880 - loss: 0.6935\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "\n",
    "vocab_size = 5001 # example value, adjust as per your dataset\n",
    "max_sequence_length = 737  # example value, adjust as necessary\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=32, input_length=max_sequence_length),\n",
    "    LSTM(32, dropout=0.2, recurrent_dropout=0.2),  # LSTM layer with dropout\n",
    "    Dense(256, activation='relu'),  # Dense layer with 256 ReLU units\n",
    "    Dropout(0.2),  # Dropout layer after dense layer\n",
    "    Dense(1, activation='sigmoid') \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    padded_sequences,  \n",
    "    train_labels,           \n",
    "    epochs=10,        \n",
    "    batch_size=10,   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "449625e5-6876-4747-b8c5-5bd682a2ca72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8316 - loss: 0.6915\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8132 - loss: 0.6916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6930835247039795, 0.5016666650772095]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(padded_sequences, train_labels)\n",
    "model.evaluate(padded_sequences_test, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bcd94-db49-44ad-b892-d61247e6c167",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "1. tf.keras.preprocessing.text.Tokenizer:https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "2. Embedding layer: https://keras.io/api/layers/core_layers/embedding/\n",
    "3. Multi-Layer Perceptron Learning in Tensorflow: https://www.geeksforgeeks.org/multi-layer-perceptron-learning-in-tensorflow/\n",
    "4. GhatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c00f9-b9a6-4679-8198-3dc3daf56ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
